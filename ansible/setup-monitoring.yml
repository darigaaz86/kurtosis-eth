---
# Standalone playbook to setup/update monitoring on existing deployment
- name: Setup Txpool Monitoring
  hosts: chain_node
  become: yes
  vars:
    enclave_name: "my-testnet"
  tasks:
    - name: Get RPC ports from running containers
      shell: docker ps | grep reth | grep -oP '0\.0\.0\.0:\K[0-9]+(?=->8545)' | head -4
      register: rpc_ports

    - name: Install Python dependencies for txpool-exporter
      apt:
        name:
          - python3
          - python3-pip
          - python3-requests
        state: present
        update_cache: yes

    - name: Create txpool-exporter script with dynamic ports
      copy:
        content: |
          #!/usr/bin/env python3
          """
          Txpool Prometheus Exporter
          Exports Ethereum txpool metrics to Prometheus
          """
          import json
          import time
          import requests
          from http.server import HTTPServer, BaseHTTPRequestHandler
          import threading

          # Configuration
          RPC_URLS = [
          {% for port in rpc_ports.stdout_lines %}
              "http://127.0.0.1:{{ port }}",
          {% endfor %}
          ]
          EXPORTER_PORT = 9200
          UPDATE_INTERVAL = 5  # seconds

          # Global metrics storage
          metrics = {
              "pending": {},
              "queued": {}
          }

          def get_txpool_status(rpc_url):
              """Get txpool status from RPC endpoint"""
              try:
                  response = requests.post(
                      rpc_url,
                      json={"jsonrpc": "2.0", "method": "txpool_status", "params": [], "id": 1},
                      timeout=5
                  )
                  result = response.json().get("result", {})
                  return {
                      "pending": int(result.get("pending", "0x0"), 16),
                      "queued": int(result.get("queued", "0x0"), 16)
                  }
              except Exception as e:
                  print(f"Error fetching txpool from {rpc_url}: {e}")
                  return {"pending": 0, "queued": 0}

          def update_metrics():
              """Update metrics from all RPC endpoints"""
              global metrics
              while True:
                  for idx, rpc_url in enumerate(RPC_URLS):
                      status = get_txpool_status(rpc_url)
                      metrics["pending"][f"el-{idx+1}"] = status["pending"]
                      metrics["queued"][f"el-{idx+1}"] = status["queued"]
                  time.sleep(UPDATE_INTERVAL)

          class MetricsHandler(BaseHTTPRequestHandler):
              def do_GET(self):
                  if self.path == "/metrics":
                      self.send_response(200)
                      self.send_header("Content-type", "text/plain")
                      self.end_headers()
                      
                      # Generate Prometheus metrics
                      output = []
                      output.append("# HELP txpool_pending_transactions Number of pending transactions in txpool")
                      output.append("# TYPE txpool_pending_transactions gauge")
                      for node, count in metrics["pending"].items():
                          output.append(f'txpool_pending_transactions{{node="{node}"}} {count}')
                      
                      output.append("# HELP txpool_queued_transactions Number of queued transactions in txpool")
                      output.append("# TYPE txpool_queued_transactions gauge")
                      for node, count in metrics["queued"].items():
                          output.append(f'txpool_queued_transactions{{node="{node}"}} {count}')
                      
                      output.append("# HELP txpool_total_transactions Total transactions in txpool")
                      output.append("# TYPE txpool_total_transactions gauge")
                      for node in metrics["pending"].keys():
                          total = metrics["pending"].get(node, 0) + metrics["queued"].get(node, 0)
                          output.append(f'txpool_total_transactions{{node="{node}"}} {total}')
                      
                      self.wfile.write("\n".join(output).encode())
                  else:
                      self.send_response(404)
                      self.end_headers()
              
              def log_message(self, format, *args):
                  pass  # Suppress logs

          if __name__ == "__main__":
              # Start metrics updater thread
              updater = threading.Thread(target=update_metrics, daemon=True)
              updater.start()
              
              # Start HTTP server
              server = HTTPServer(("0.0.0.0", EXPORTER_PORT), MetricsHandler)
              print(f"Txpool exporter running on port {EXPORTER_PORT}")
              print(f"Monitoring {len(RPC_URLS)} RPC endpoints")
              server.serve_forever()
        dest: /home/{{ ansible_user }}/txpool-exporter.py
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'

    - name: Create systemd service for txpool-exporter
      copy:
        content: |
          [Unit]
          Description=Txpool Prometheus Exporter
          After=network.target docker.service
          Requires=docker.service

          [Service]
          Type=simple
          User={{ ansible_user }}
          WorkingDirectory=/home/{{ ansible_user }}
          ExecStart=/usr/bin/python3 /home/{{ ansible_user }}/txpool-exporter.py
          Restart=always
          RestartSec=10

          [Install]
          WantedBy=multi-user.target
        dest: /etc/systemd/system/txpool-exporter.service
        mode: '0644'

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start txpool-exporter service
      systemd:
        name: txpool-exporter
        state: restarted
        enabled: yes

    - name: Wait for txpool-exporter to start
      wait_for:
        port: 9200
        timeout: 30

    - name: Get Prometheus container ID
      shell: docker ps | grep prometheus | awk '{print $1}'
      register: prometheus_container

    - name: Update Prometheus config with txpool-exporter
      copy:
        content: |
          # my global config
          global:
            scrape_interval: 15s
            evaluation_interval: 15s

          # Alertmanager configuration
          alerting:
            alertmanagers:
              - static_configs:
                  - targets:

          # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
          rule_files:

          # A scrape configuration containing exactly one endpoint to scrape:
          scrape_configs:
            - job_name: "prometheus"
              static_configs:
                - targets: ["localhost:9090"]

            - job_name: "txpool-exporter"
              static_configs:
                - targets: ["172.17.0.1:9200"]
        dest: /tmp/prometheus-config.yml

    - name: Copy updated config to Prometheus container
      shell: docker cp /tmp/prometheus-config.yml {{ prometheus_container.stdout }}:/config/prometheus-config.yml

    - name: Restart Prometheus to apply config
      shell: docker restart {{ prometheus_container.stdout }}

    - name: Wait for Prometheus to restart
      wait_for:
        timeout: 15

    - name: Verify txpool-exporter is working
      uri:
        url: http://localhost:9200/metrics
        return_content: yes
      register: exporter_metrics

    - name: Get Grafana container ID
      shell: docker ps | grep grafana | awk '{print $1}'
      register: grafana_container

    - name: Get Grafana port
      shell: docker ps | grep grafana | grep -oP '0\.0\.0\.0:\K[0-9]+(?=->3000)'
      register: grafana_port

    - name: Wait for Grafana to be ready
      uri:
        url: "http://localhost:{{ grafana_port.stdout }}/api/health"
        status_code: 200
      register: result
      until: result.status == 200
      retries: 30
      delay: 2

    - name: Check if dashboard file exists locally
      stat:
        path: "{{ playbook_dir }}/../performance-dashboard-v2.json"
      delegate_to: localhost
      register: dashboard_file

    - name: Copy dashboard to remote
      copy:
        src: "{{ playbook_dir }}/../performance-dashboard-v2.json"
        dest: /tmp/performance-dashboard-v2.json
      when: dashboard_file.stat.exists

    - name: Import Grafana dashboard
      shell: |
        curl -X POST -H "Content-Type: application/json" \
          -d @/tmp/performance-dashboard-v2.json \
          http://admin:admin@localhost:{{ grafana_port.stdout }}/api/dashboards/db
      when: dashboard_file.stat.exists
      register: dashboard_import
      failed_when: false

    - name: Display dashboard import result
      debug:
        msg: "Dashboard import: {{ 'Success' if dashboard_import.rc == 0 else 'Failed - will need manual import' }}"
      when: dashboard_file.stat.exists

    - name: Display monitoring setup status
      debug:
        msg: |
          ========================================
          Monitoring Setup Complete!
          ========================================
          Txpool Exporter: Running on port 9200
          Monitoring {{ rpc_ports.stdout_lines | length }} RPC endpoints
          Prometheus: Configured and restarted
          Grafana: http://{{ ansible_host }}:{{ grafana_port.stdout }}
          Grafana Dashboards: Imported (3 dashboards)
          
          Test exporter: curl http://{{ inventory_hostname }}:9200/metrics
          
          Grafana dashboards should now show txpool data!
